---
title: "DADA2 Tutorial: Durban 2018"
output: slidy_presentation
---

Adapted from Ben C. Callahan [benjjneb](https://github.com/benjjneb).

## Processing 16S rRNA marker-gene data with dada2.

**This workflow assumes that your sequencing data meets certain criteria:**

- Samples have been demultiplexed, i.e. split into individual per-sample fastq files.

- Non-biological nucleotides have been removed, e.g. primers, adapters, linkers, etc.

- If paired-end sequencing data, the forward and reverse fastq files contain reads in matched order.

**Additional information about demultiplexing can be found on the dada2 website: https://benjjneb.github.io/dada2/index.html

## Load package and set path

Load the `dada2` package. If you don't already it, see the [dada2 installation instructions](dada-installation.html):

```{r load-libraries}
library(dada2); packageVersion("dada2")

```

# Set the path to the fastq files:

```{r path}
# You will need to make sure the path corresponds to where you cloned the repository
path <- "~/Dropbox/Research/2018_ASM_Workshop/data/fastqs/"
list.files(path)

```

# Forward, Reverse, Sample Names

Get matched lists of the forward and reverse fastq.gz files:

```{r filenames}
# Forward and reverse fastq filenames have format: SAMPLENAME_R1.fastq.gz and SAMPLENAME_R2.fastq.gz
fnFs <- sort(list.files(path, pattern="_R1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2.fastq.gz", full.names = TRUE))
fnFs[[1]]; fnRs[[1]]

```

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq.gz:

```{r sample-names}
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
sample.names

```

## Check your amplicon design

We are using the 515F/806R primer set. The primers are not sequenced. The sequencing technology is 2x250 paired end Illumina (MiSeq).

## Inspect forward read quality profiles

```{r plotqF}
plotQualityProfile(fnFs[c(1,11)])

```

**Where to truncate?**

## Inspect reverse read quality profiles

```{r plotqR}
plotQualityProfile(fnRs[c(2,12)])

```

## Filter and trim

Assign filenames for the filtered fastq.gz in the filtered/ subdirectory.

```{r filt-names}
# This presets output file names for the filtered data
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))

```

*What values will you select for quality control?*

```{r filter, message=FALSE, warning=FALSE}
out <- filterAndTrim(fwd = fnFs, filt = filtFs, rev = fnRs, filt.rev = filtRs, 
                     truncLen=c(240,170),
                     maxEE=c(2,2),
                     compress=TRUE,
                     multithread=FALSE)

```

## SANITY CHECK: Filtering Stats

```{r filter-stats}
head(out)

# Let's recall our ggplot2 exercises and make a quick plot of the trimming results in 'out'
# First we need to load the ggplot library
library(tidyverse)

# Check what type of object "out" is using the 'class' command
class(out)

# ggplot doesn't work on matrixes, so you need to conver "out" to a data.frame
# using 'class' to determine what type of object you are working with can solve numerous problems

out.df <- as.data.frame(out)

ggplot(out.df, aes(x = reads.in, y = reads.out)) +
  geom_point(size = 3, alpha = 0.6) +
  geom_smooth(method = "lm") + # quick linear model
  labs(x = "Raw Reads", y = "Trimmed Reads", title = "Read Trimming Results")

# What other data can we extract from this table?
# Just some examples of how to calculate quick table statstics
out.df %>%
  mutate(diff = reads.in-reads.out) %>%
  mutate(percent = round((100*(reads.out/reads.in)), digits = 2))

```

- What fraction of reads were kept?
- Was that fraction reasonably connsistent among samples?
- Were enough reads kept to achieve your analysis goals?

**The truncation lengths are the most likely parameters you might want to revisit.**

# Learn the Error Rates

```{r learn-errors}
# Learn the error rates for the forward reads
errF <- learnErrors(filtFs, multithread=1) # Set multithread=TRUE to use all cores

# Learn the error rates for the reverse reads
errR <- learnErrors(filtRs, multithread=1)

# There is a 'hidden' function in dada2 that allows you to extract the 'convergence' estimate at each iteration
plot(dada2:::checkConvergence(errF), type = "o", col = "firebrick3", main = "Convergence")

# Don't worry too much about this, as long as your convergence (y-axis) is flattening out over the iterations (x-axis) then you are in good shape

```

The DADA2 algorithm makes use of a parametric error model (`err`) and every amplicon dataset has a different set of error rates. The `learnErrors` method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution.

## SANITY CHECK: Error Rates

```{r plot-errors, warning=FALSE}
plotErrors(errF, nominalQ=TRUE)

```

Check error model estimation convergence. The line should rapidly drop-off near zero and not substantially improve thereafter.

```{r plot-convergence}
plot(dada2:::checkConvergence(errF))

```

- Does the model (black line) reasonably fit the observations (black points)?
- Do the error rates mostly decrease with quality score?

The goal here is good, not perfect, so don't sweat the small stuff (or non-convergence).

## Dereplicate

Dereplication combines all identical sequencing reads into "unique sequences" with a corresponding "abundance" equal to the number of reads with that unique sequence.

```{r dereplicate, message=FALSE}
derepFs <- derepFastq(filtFs)
derepRs <- derepFastq(filtRs)

# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

```

**Big Data**: The tutorial dataset is small enough to easily load into memory. If your dataset exceeds available RAM, it is preferable to process samples one-by-one in a streaming fashion: see the [DADA2 Workflow on Big Data](bigdata.html) for an example.

## Sample Inference

We are now ready to apply [the core sample inference algorithm](https://www.nature.com/articles/nmeth.3869#methods) to the dereplicated data. 

```{r dada}
dadaFs <- dada(derepFs, err=errF, multithread=1) # Set multithread=TRUE to use all cores
dadaRs <- dada(derepRs, err=errR, multithread=1)

```

Inspecting the returned `dada-class` object:

```{r see-dada}
dadaFs[[1]]

# The key thing to note here is the number of sequence variants identified in the input sequences

```
## DADA OPTIONS: Pooling and Pyrosequencing

# Pooling
Pooling can [increase sensitivity to rare per-sample variants](https://benjjneb.github.io/dada2/pool.html#pooling-for-sample-inference). Pseudo-pooling [approximates pooling in linear time](https://benjjneb.github.io/dada2/pseudo.html#pseudo-pooling).

# Pyrosequencing
For pyrosequencing data (e.g. 454 or Ion Torrent) we recommend a slight change in the alignment parameters to better handle those technologies tendency to make homopolymer errors.

```{r pyro, eval=FALSE}
# THIS IS JUST AN EXAMPLE OF THE SETTINGS ONE WOULD WANT TO USE FOR PYROSEQUENCED DATA

# DO NOT RUN AS PART OF THIS LAB

# foo <- dada(..., HOMOPOLYMER_GAP_PENALTY=-1, BAND_SIZE=32)
```

*The adventurous can see `?setDadaOpt` for more algorithmic parameters.*

## Merge Paired Reads

```{r merge, message=FALSE}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)

```

It is arguable if merging adds anything to the analysis for this primer set. Since R1 (fwd) and R2 (rev) sequence an overlapping region of the 16S rRNA gene, just analyzing R1 is essentially the same as analyzing merged R1-R2.

Other primer sets of genes (ITS, 18S, etc.) might benefit from merging.

**Most reads should pass the merging step! If that isn't the case, are you sure your truncated reads still overlap sufficiently?**

## Construct Sequence Table (ASV Table)

```{r seqtab}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

```

The sequence table is a `matrix` with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants.

## Remove chimeras

Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant "parent" sequences.

```{r chimeras, message=FALSE}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=1, verbose=TRUE)
# Set multithread=TRUE to use all cores

# Calculate % of reads that pass chimera filtering
100*(sum(seqtab.nochim)/sum(seqtab))

```

## Track reads through the pipeline

Look at the number of reads that made it through each step in the pipeline:

```{r track}
# This is some custom code to pull out important values from the dada2 workflow
# Don't worry too much about the code at this point, just focus on teh results table (track)
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "nonchim")
rownames(track) <- sample.names
head(track)

# Is this a matrix or data.frame?
class(track)

# It's a matrix again. Let's convert and make some quick plots
track.df <- as.data.frame(track)

# ggplot2 (and all of tidyverse) works really well with 'long' form data
# Let's conver to long form so we can make a line plot of each stage
track.df.long <- track.df %>%
  rownames_to_column(var = "sample_id") %>% # make the rownames a variable
  pivot_longer(-sample_id, # Pivot everything other than sample_id
               values_to = "Number", # Column name the values
               names_to = "Stage") # Column name the stages

# Now we can plot the number of reads per each stage
# There are likely some new features in this ggplot2 code
# Google them or read the help pages to see what they do!

# Line plot
ggplot(track.df.long, aes(x = reorder(sample_id, Number), y = Number, col = Stage, group = Stage)) +
  theme_linedraw() +
  geom_point(size = 2) +
  geom_line(lty = 2) +
  theme(axis.text.x = element_blank())

# Boxplot
ggplot(track.df.long, aes(x = reorder(Stage, -Number), y = Number)) +
  geom_boxplot(outlier.shape = "") +
  geom_jitter(width = 0.1, alpha = 0.7, size = 2)

# You can also quickly get summary statistcs from long form data
track.df.long %>%
  group_by(Stage) %>%
  summarise(Mean = sum(Number))

```

Looks good! We kept the majority of our raw reads, and there is no over-large drop associated with any single step.

- If a majority of reads failed to merge, you may need to revisit `truncLen` to ensure overlap.
- If a majority of reads were removed as chimeric, you may have unremoved primers.

## Assign Taxonomy

The `assignTaxonomy` function takes as input a set of sequences to ba classified, and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least `minBoot` bootstrap confidence.

This process can be slow, so we have precomputed this. The command you would actually run is #'d out in the following chunk. Do not run this, but DO run the unhased portion to load the precomputed taxonomic assignment file.

```{r taxify}
### Commented out for cloud-friendliness
### taxa <- assignTaxonomy(seqtab.nochim, "../data/rdp_train_set_16.fa.gz", multithread=2)

taxa <- readRDS(file.path(path, "..", "taxa.rds"))

```

**Do the taxonomies assigned to the top ASVs make sense in the sampled environment?**

## Handoff to Phyloseq

```{r phyloseq}
library("phyloseq"); packageVersion("phyloseq")

```

Create a phyloseq object from the ASV table and taxonomy assigned by DADA2.

```{r make-ps}
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               tax_table(taxa))
ps

# Some sanity checks
nsamples(ps)
get_taxa_unique(ps, "Family")

```