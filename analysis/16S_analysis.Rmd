---
title: "2018 ASM Using R to Analyze the Bacterial Microbiome Workshop"
author: "Scott A. Handley"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: html_document
---

# Background

Generalized workflow for processing 16S rRNA gene amplicon data. Each R chunk represents a specific analysis type.

The code chunks perform the following functions:

1) Environment initiation
2) Data loading
3) Variable examination and modification
4) Data assessment
5) Taxon prevalence estimations and filtering
6) Data transformation
7) Subsetting
8) Community composition plotting
9) Alpha diversity analysis
10) Beta diversity analysis
11) Differential abundance testing

The data originate from a study on the bacterial microbiome of mice treated with or without antibiotics to test the affects of the microbiome on flavivirus infection (https://www.ncbi.nlm.nih.gov/pubmed/29590614). Sequence data was generated from extracted nucleic acid from stool samples collected from individually caged mice and amplified using primers specific for the V4 region using primers 515F/806R.

The study followed flavivirus infection after the following treatments:

1) Koolaid: Antibiotics are provided to the mice via their drinking water. As many of the antibiotics taste bad, koolaid is added as a sweetener. Therefore, the appropriate control is water spike with koolaid.
2) Ampicillin (Amp): https://en.wikipedia.org/wiki/Ampicillin
3) Metronidazole (Met): https://en.wikipedia.org/wiki/Metronidazole
4) Ampicillin + Metronidazole (Amp+Metro)

Treatments was supplied for 2 weeks prior to viral infection and maintained for 2 weeks post-infection. Primary outcome was mouse survival. Each antibiotic treatment group had two subgroups of mice that were either a) left uninfected as controls or b) infected with West Nile Virus via a subcutaneous footpad injection.

## Data organization:

There is no single way to organize your data, but it is good practice to develop a standard for file storage and naming. An well thought out organization structure will make coding your analysis easier, enable you to more easily return to your data after leaving it for a period of time and facilitate publication of your data.

The organizational scheme for this workshop is as follow:

General rules for file naming and organization:

1) Special characters and spaces in file or directory names other than _ and - are evil
2) Naming parity is useful. For example, if you have a data object called 16S_analysis.RDS, then it would be helpful to have the same prefix for associated files (e.g. 16S_mapping.txt, 16S_data.Rmd, 16S_project.RProj)

For the purposes of this Workshop we have arranged directories and files as follows:

1) All files are in the base directory called /2018_ASM_Workshop/
2) Primary data files are in /data/
3) Analysis files (typically RMarkdown documents) are in /analysis/
3) Results are in /results/
4) Figures are in /figures/

## Data and environment initiation

We will begin by customizing our global settings, activating packages and loading our data into R using the following steps:

1) Set global knitr options
2) Load libraries
3) Set global ggplot2 theme and options
4) Load data

### Set global knitr options

Knitr is a standardized library which "knits" together code chunks and converts them to specified format such as HTML or PDF. This is very useful for report generation. The way in which knitr handles chunk formatting and report generation can be specified in a code chunk. There are a number options you can use in this section [read about here](https://yihui.name/knitr/options/).

```{r global_options, include=FALSE}
# This chunk defines output figure dimensions,
# specifies a path where knitted figures will reside after knitting, 
# and prevents display of warnings in the knitted report
knitr::opts_chunk$set(fig.width=8,
                      fig.height=6,
                      fig.path="../figures/",
                      dev='png',
                      warning=FALSE,
                      message=FALSE)
```

## Load libraries

```{r initiate-environment}
library("tidyverse"); packageVersion("tidyverse")
library("plyr"); packageVersion("plyr")
library("phyloseq"); packageVersion("phyloseq")
library("vegan"); packageVersion("vegan")
library("gridExtra"); packageVersion("gridExtra")
library("knitr"); packageVersion("knitr")
library("DESeq2"); packageVersion("DESeq2")
library("plotly"); packageVersion("plotly")
library("microbiome"); packageVersion("microbiome")
library("ggpubr"); packageVersion("ggpubr")
library("data.table"); packageVersion("data.table")

```

## Set global ggplot2 theme and options.

This sets the plotting aesthetics for every ggplot2 for the rest of the document. There are a tremendous number of ways to customize your ggplot2 settings using theme_set. It is best practice to do this at the beginning of the RMarkdown document so that these settings propagate to the entirety of the studies plots.

```{r global-theme-settings, include=FALSE}
# Set global theming
# This theme set will change the ggplot2 defaults to use the b&w settings (removes the default gray background) and sets the devault font to 10pt Helvetica.
theme_set(theme_bw(base_size = 10,
                   base_family = "Arial"))

```

Of note, there are a number of ways to customize R code chunks. For the knitr and ggplot2 theme settings I have decided to set include=FALSE. This tells knitr to exclude the chunk from the final report. In this case, the chunk will still be evaluated as part of the RMarkdown document. If you wish to prevent the chunk from being executed you can set eval=FALSE.

## Read in your data

The output from a standard dada2 workflow should be an RDS file. In this case the file is called *ps0.rds* (ps is shorthand for PhyloSeq. The 0 indicates it is the 'base' version of the file. As it is modified this can be changed to ps1, ps2, etc.). You may have already merged your mapping file data (sample variables) with the rds file. However, you will likely add or modify this mapping file as you progress, so it is useful to initiate an import/merge of a mapping file at this stage.

```{r initiate-data}
# Read in an RDS file containing taxonomic and count information
ps0 <- readRDS("../data/16S_data.RDS")
ps0

# Read in a mapping file containing sample variable information
map <- import_qiime_sample_data("../data/16S_mapping.txt")
dim(map)

# Merge the RDS file with the mapping file
ps0 <- merge_phyloseq(ps0, map)

# Perform a few sanity checks
sample_variables(ps0) # Display variables from the mapping file
ntaxa(ps0) # Total number of taxa in the entire data
rank_names(ps0) # Taxonomic ranks
get_taxa_unique(ps0, "Phylum") # Unique phylum names in the file

```

## Sample filtering

```{r sample-filtering}
# Remove Day -14 cohoused data
# These samples were collected and sequenced, but were obtained prior to mouse co-housing and thus not included in subsequent analysis
levels(sample_data(ps0)$treatment_days)
ps0 <- subset_samples(ps0, treatment_days != "D.14")
levels(sample_data(ps0)$treatment_days)

# A group of uninfected animals were collected as well, but not analyzed as part of this study
levels(sample_data(ps0)$virus)
ps0 <- subset_samples(ps0, virus == "WNV2000")
levels(sample_data(ps0)$virus)

# Remove taxa no longer part of the count table due to sample removal
summary(taxa_sums(ps0))
ps0 <- prune_taxa(taxa_sums(ps0) > 0, ps0)
summary(taxa_sums(ps0))

```

## Factor reordering and renaming (optional)

The default sorting for ggplot2 is alphabetical. So if you want to make a box plot comparing Shannon diversity between wild-type and knockout mice, it will by default always place knockout on the left and wild-type on the right. However, you may wish to switch this so the knock-out is on the right and wild-type on the left.

This can be done on a plot-by-plot basis, however, it is likely that you will want all of your plots to reflect this customization throughout the entire analysis, so it is useful to have an R chunk at the very beginning of your workflow to specify order and label names.

In the example data, most of the analysis will be done comparing the sample variable "treatment" which is either KoolAid or Ampicillin in the mapping file. Due to default ordering, Ampicillin will always appear before Koolaid. We want the control displayed first (on the left of most plots). We also want to use the more formal "Vehicle" to indicate that a "vehicle control" was used. Koolaid is added to the water to encourage mice to drink the antibiotic laden water. This would be indicated in the methods of a manuscript, but the plots should be more formal and indicate that this was a vehicle control. The code chunk below provides examples for reordering and relabeling sample variable data.

```{r factor-adjustments}
# Reorder Treatments
levels(sample_data(ps0)$treatment)
sample_data(ps0)$treatment <- factor(sample_data(ps0)$treatment, levels = c("Vehicle","Metro","Amp","AmpMetro"))
levels(sample_data(ps0)$treatment)

# Relabel Treatments
sample_data(ps0)$treatment <- factor(sample_data(ps0)$treatment, labels = c("Vehicle","Metro","Amp","Amp + Metro"))
levels(sample_data(ps0)$treatment)

# Factor re-ordering, relabelling, etc.
# Reorder Time points
levels(sample_data(ps0)$treatment_days)
sample_data(ps0)$treatment_days <- factor(sample_data(ps0)$treatment_days, levels = c("D0", "D3", "D7", "D13", "D16", "D18", "D20"))
levels(sample_data(ps0)$treatment_days)

```

## ASV summary statistics

Data assessment consists of 2 steps:

1) Evaluate Amplicon Sequence Variants (ASV, formerly referred to as an OTU) summary statistics
2) Detect and remove outlier samples

Begin by running the following R chunk to produce several summary plots and basic statistics about the ASV's and samples in your data.

```{r data-assessment}
# Create a new data frame of the sorted row sums, a column of sorted values from 1 to the total number of individuals/counts for each ASV and a categorical variable stating these are all ASVs.
readsumsdf <- data.frame(nreads = sort(taxa_sums(ps0), TRUE), 
                        sorted = 1:ntaxa(ps0),
                        type = "ASVs")

# Add a column of sample sums (total number of individuals per sample)
readsumsdf <- rbind(readsumsdf,
                   data.frame(nreads = sort(sample_sums(ps0), TRUE),
                              sorted = 1:nsamples(ps0),
                              type = "Samples"))

# Make a data frame with a column for the read counts of each sample for histogram production
sample_sum_df <- data.frame(sum = sample_sums(ps0))

# Make plots
# Generates a bar plot with # of reads (y-axis) for each taxa. Sorted from most to least abundant
# Generates a second bar plot with # of reads (y-axis) per sample. Sorted from most to least
p.reads = ggplot(readsumsdf, aes(x = sorted, y = nreads)) +
  geom_bar(stat = "identity") +
  ggtitle("ASV Assessment") +
  scale_y_log10() +
  facet_wrap(~type, scales = "free") +
  ylab("# of Sequences")

# Histogram of the number of Samples (y-axis) at various read depths
p.reads.hist <- ggplot(sample_sum_df, aes(x = sum)) + 
  geom_histogram(color = "black", fill = "firebrick3", binwidth = 150) +
  ggtitle("Distribution of sample sequencing depth") + 
  xlab("Read counts") +
  ylab("# of Samples")

# Final plot, side-by-side
grid.arrange(p.reads, p.reads.hist, ncol = 2)

# Basic summary statistics
summary(sample_sums(ps0))

```
The above data assessment is useful for getting an idea of 1) the overall taxonomic distribution of your reads (left plot). This will normally be a "long tail" with some taxa being highly abundant in the data tapering off to taxa with very few reads, 2) probably more valuable than the first plot is how many reads are in each sample (middle plot). Very low read count can be indicative of a failed reaction and 3) a histogram of the number of samples at various "bins" of read depth. Each of these plots will help give an understanding of how your data are structured across taxa and samples and will vary depending on the nature of your samples.

Samples with unexpectedly low number of sequences can be safely removed. This is an intuitive process and should be instructed by your understanding of the samples in your study. For example, if you have 5 samples from stool samples, one would expect to obtain thousands, if not several thousands of RSVs. This may not be the case for other tissues, such as spinal fluid or tissue samples. Similarly, you would not expect thousands of RSV from samples obtained from antibiotic treated organisms. Following antibiotic treatment you may be left with dozens or hundreds of RSVs. So contextual awareness about the biology of your system should guide your decision to remove samples based on RSV number. The basic idea is to remove samples with "unexpected" numbers of RSV.

Importantly, at each stage you should document and justify your decisions. If you are concerned that sample removal will alter the interpretation of your results, you should run your analysis on the full data and the data with the sample(s) removed to see how the decision affects your interpretation.

The above plots provide overall summaries about the number of RSVs found in all of your samples. However, they are not very useful for identifying and removing specific samples. This can be done using the following R chunk.

```{r sample-removal-identification}
# Format a data table to combine sample summary data with sample variable data
ss <- sample_sums(ps0)
sd <- as.data.frame(sample_data(ps0))
ss.df <- merge(sd, data.frame("ASV" = ss), by ="row.names")

# Plot the data by the treatment variable
y = 1000 # Set a threshold for the minimum number of acceptable reads. Can start as a guess
x = "treatment_days" # Set the x-axis variable you want to examine
label = "sample" # This is the label you want to overlay on the points that are below threshold y. Should be something sample specific

p.ss.boxplot <- ggplot(ss.df, aes_string(x, y = "ASV", color = "treatment")) + 
  stat_boxplot(geom = "errorbar", position = position_dodge(width = 0.8)) +
  geom_boxplot(outlier.colour="NA", position = position_dodge(width = 0.8)) +
  geom_jitter(size = 2, alpha = 0.6) +
  scale_y_log10() +
  facet_wrap(~treatment) +
  geom_hline(yintercept = y, lty = 2) +
  geom_text(aes_string(label = label), size = 3, nudge_y = 0.05, nudge_x = 0.05)
p.ss.boxplot

write.table(ss.df, file = "../results/asv_stats.txt", sep = "\t")

```
The example data does have a couple of samples with fewer than 1,000 ASVs. However, these come from samples obtained from antibiotic treated mice, so this fits our expectation. There are a 7 samples in the Amp + Metro treated mice at the later time points that seem to be performing differently (very low numbers of ASV) in comparison to the majority of samples. When questionable samples arise you should take note of them so if there are samples which behave oddly in downstream analysis you can recall this information and perhaps justify their removal. In this case lets remove them. 

```{r sample-outlier-removal}
nsamples(ps0)
ps1 <- ps0 %>%
  subset_samples(
    sample != "340" &
    sample != "405" &
    sample != "402" &
    sample != "468" &
    sample != "470" &
    sample != "535" &
    sample != "532" &
    sample != "208" &
    sample != "275" &
    sample != "274"
)
nsamples(ps1)

```

## Overall sample relationship to evaluate sample outliers

Note that we created a new phyloseq object called ps1. This preserves all of the data in the original ps0 and creates a new data object with the offending sample(s) removed called ps1.

Failure to detect and remove "bad" samples can make interpreting ordinations much more challenging as they typically project as "outliers" severely skewing the rest of the samples. These samples also increase variance and will impede your ability to identify differentially abundant taxa between groups. So sample outlier removal should be a serious and thoughtful part of every analysis in order to obtain optimal results.

## Taxon prevalence estimations and filtering

Low abundant taxa typically do not contribute to ecological community evaluation or differential abundance testing. There are of course caveats to this statement (i.e. low-abundance pathogen detection), but many analysis can benefit from the removal of uninformative (low prevalence) taxa. Removal of low prevalence taxa greatly assist in tests penalized with a false-discovery-rate (FDR) calculation. Similar to outlier sample removal, low prevalent taxa removal should be justified and documented. The following R chunk provides several evaluations and plots to assist with this decision.

## Taxon cleaning 

```{r taxon-cleaning}
# Begin by removing sequences that were not classified as Bacteria or were classified as either mitochondria or chloroplast

ps1 # Check the number of taxa prior to removal
ps2 <- ps1 %>%
  subset_taxa(
    Kingdom == "Bacteria" &
    Family  != "mitochondria" &
    Class   != "Chloroplast" &
    Phylum != "Cyanobacteria/Chloroplast"
  )
ps2 # Confirm that the taxa were removed

```

## Subsetting

You will frequently find that you want to analyze a subset of your total data set. There are typically commands that will allow you to do this for each individual analysis, but similar to variable reordering it can sometime be more convenient to do this towards the beginning of your analysis. This should be done after removal of outlier samples and taxa. If you wish to create transformed versions of each subset you can either subset the transformed data you just generated, or alternatively retransform your subsetted data. The R chunk below is an example subsetting of the example data by treatment.

Subsetting away samples can create a situation where taxa are present as empty rows. This is because not every sample has every taxa. These can be removed as shown in the R chunk below.

Creating individual subsets like this can be particularly useful when assessing differential abundance using DESeq2.

```{r subsetting, include=FALSE}
#Subsets
# All samples
ntaxa(ps2)
ps2 <- prune_taxa(taxa_sums(ps2) > 0, ps2)
ntaxa(ps2)

# Vehicle
ps2
ps2.vehicle <- subset_samples(ps2, treatment == "Vehicle")
any(taxa_sums(ps2.vehicle) == 0) # In this case it is TRUE, so remove the zero's
ps2.vehicle <- prune_taxa(taxa_sums(ps2.vehicle) > 0, ps2.vehicle)
any(taxa_sums(ps2.vehicle) == 0) # It should now be false

# Amp
ps2
ps2.amp <- subset_samples(ps2, treatment == "Amp")
any(taxa_sums(ps2.amp) == 0) # In this case it is TRUE, so remove the zero's
ps2.amp <- prune_taxa(taxa_sums(ps2.amp) > 0, ps2.amp)
any(taxa_sums(ps2.amp) == 0) # It should now be false

# Metro
ps2
ps2.metro <- subset_samples(ps2, treatment == "Metro")
any(taxa_sums(ps2.metro) == 0) # In this case it is TRUE, so remove the zero's
ps2.metro <- prune_taxa(taxa_sums(ps2.metro) > 0, ps2.metro)
any(taxa_sums(ps2.metro) == 0) # It should now be false

# Amp Metro
ps2
ps2.ampmetro <- subset_samples(ps2, treatment == "Amp + Metro")
any(taxa_sums(ps2.ampmetro) == 0) # In this case it is TRUE, so remove the zero's
ps2.ampmetro <- prune_taxa(taxa_sums(ps2.ampmetro) > 0, ps2.ampmetro)
any(taxa_sums(ps2.ampmetro) == 0) # It should now be false

```

## Community composition plotting

```{r community-composition-plots}
# Create a data table for ggploting
ps2_phylum <- ps2 %>%
  tax_glom(taxrank = "Phylum") %>%                     # agglomerate at phylum level
  transform_sample_counts(function(x) {x/sum(x)} ) %>% # Transform to rel. abundance (or use ps0.ra)
  psmelt() %>%                                         # Melt to long format for easy ggploting
  filter(Abundance > 0.01)                             # Filter out low abundance taxa

# Plot - Phylum
p.ra.phylum <- ggplot(ps2_phylum, aes(x = sample_id, y = Abundance, fill = Phylum)) + 
  geom_bar(stat = "identity", width = 1) +
  facet_wrap(treatment~treatment_days, scales = "free_x", nrow = 4, ncol = 7) +
  theme(axis.text.x = element_blank()) +
  theme(axis.title.x = element_blank()) +
  labs(title = "Abundant Phylum (> 1%)")
p.ra.phylum

# Note: This is a nice place to output tables of data that you may want to use for other analysis, or to include as supplemental data for publication
# You can rerun the first bit of code in this chunk and change Phylum to Species for a table with all possible classifications
write.table(ps2_phylum, file = "../results/phylum_relab.txt", sep = "\t")

# Draw in interactive plotly plot
ggplotly(p.ra.phylum)

```

```{r prevalence-assessment}
# Prevalence estimation
# Calculate feature prevalence across the data set
prevdf <- apply(X = otu_table(ps2),MARGIN = ifelse(taxa_are_rows(ps2), yes = 1, no = 2),FUN = function(x){sum(x > 0)})

# Add taxonomy and total read counts to prevdf
prevdf <- data.frame(Prevalence = prevdf, TotalAbundance = taxa_sums(ps2), tax_table(ps2))

# Create a table of Phylum, their mean abundances across all samples, and the number of samples they were detected in
plyr::ddply(prevdf, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})

#Prevalence plot
prevdf1 <- subset(prevdf, Phylum %in% get_taxa_unique(ps0, "Phylum"))
p.prevdf1 <- ggplot(prevdf1, aes(TotalAbundance, Prevalence / nsamples(ps2),color=Family)) +
  geom_hline(yintercept = 0.05, alpha = 0.5, linetype = 2) +
  geom_point(size = 3, alpha = 0.7) +
  scale_x_log10() +
  xlab("Total Abundance") + ylab("Prevalence [Frac. Samples]") +
  facet_wrap(~Phylum) +
  theme(legend.position="none") +
  ggtitle("Phylum Prevalence in All Samples\nColored by Family")
p.prevdf1

```
This code will produce a table and a plot of all of the Phyla present in your samples along with information about their prevalence (fraction of samples they are present in) and total abundance across all samples. 

...ADD ADDITIONAL EXPLANATION AND STRATEGY HERE...

Example on how to filter low prevalent taxa below. Not used for the original analysis though, need to consider to include or not and update example below.

```{r prevalence-filtering-example, eval=FALSE, include=FALSE}
# Remove specific taxa
# Define a variable with taxa to remove
filterPhyla = c("Fusobacteria", "Tenericutes")
filterPhyla

get_taxa_unique(ps2, "Phylum") # Check the number of taxa prior to removal
ps2.prev <- subset_taxa(ps2, !Phylum %in% filterPhyla) 
get_taxa_unique(ps2.prev, "Phylum") # Confirm the taxa were removed

# Removing taxa that fall below 5% prevelance
# Define the prevalence threshold
prevalenceThreshold = 0.05 * nsamples(ps2)
prevalenceThreshold

# Define which taxa fall within the prevalence threshold
keepTaxa <- rownames(prevdf1)[(prevdf1$Prevalence >= prevalenceThreshold)]
ntaxa(ps2) # Check the number of taxa prior to removal
ps2.prev <- prune_taxa(keepTaxa, ps2)
ntaxa(ps2.prev) # Confirm the taxa were removed

```

## Data transformation

Many analysis in community ecology and hypothesis testing benefit from data transformation. Many microbiome data sets do not fit to a normal distribution, but transforming them towards normality may enable more appropriate data for specific statistical tests. The choice of transformation is not straight forward. There is literature on how frequently used transformations affect certain analysis, but every data set may require different considerations. Therefore, it is recommended that you examine the effects of several transformations on your data and explore how they alter your results and interpretation.

The R chunk below implements several commonly used transformations in microbiome research and plots their results. Similar to outlier removal and prevalence filtering, your choice should be justified, tested and documented.

```{r data-transform, include=FALSE}
# Transform to Relative abundances
ps2.ra <- transform_sample_counts(ps2, function(OTU) OTU/sum(OTU))

# Transform to Proportional Abundance
ps2.prop <- transform_sample_counts(ps2, function(x) min(sample_sums(ps2)) * x/sum(x))

# Log transformation moves to a more normal distribution
ps2.log <- transform_sample_counts(ps2, function(x) log(1 + x))

# View how each function altered count data
par(mfrow=c(1,4))
plot(sort(sample_sums(ps2), TRUE), type = "o", main = "Native", ylab = "RSVs", xlab = "Samples")
plot(sort(sample_sums(ps2.log), TRUE), type = "o", main = "log Transformed", ylab = "RSVs", xlab = "Samples")
plot(sort(sample_sums(ps2.ra), TRUE), type = "o", main = "Relative Abundance", ylab = "RSVs", xlab = "Samples")
plot(sort(sample_sums(ps2.prop), TRUE), type = "o", main = "Proportional Abundance", ylab = "RSVs", xlab = "Samples")
par(mfrow=c(1,4))

# Histograms of the non-transformed data vs. the transformed data can address the shift to normality
p.nolog <- qplot(rowSums(otu_table(ps2))) + ggtitle("Raw Counts") +
  theme_bw() +
  xlab("Row Sum") +
  ylab("# of Samples")

p.log <- qplot(log10(rowSums(otu_table(ps2)))) +
  ggtitle("log10 transformed counts") +
  theme_bw() +
  xlab("Row Sum") +
  ylab("# of Samples")

ggarrange(p.nolog, p.log, ncol = 2, labels = c("A)", "B)"))

```

## Phyla level plots

```{r phyla-level-plots-preparation}
# agglomerate taxa
glom <- tax_glom(ps2.ra, taxrank = 'Phylum')

# create dataframe from phyloseq object
dat <- as.tibble(psmelt(glom))

# Reorder Phylum levels from most -> least abundant
levels(dat$Phylum)
dat$Phylum <- factor(dat$Phylum, levels = c("Bacteroidetes", "Firmicutes", "Proteobacteria", "Tenericutes", "Actinobacteria", "Verrucomicrobia"))
levels(dat$Phylum)

# Reduced to most abundant phylum
dat %>%
  group_by(Phylum) %>%
  summarise(Mean = mean(Abundance)) %>%
  arrange(desc(Mean))

# Select 4 most abundant Phyla
dat.1 <- filter(dat, Phylum %in% c("Bacteroidetes", "Firmicutes", "Proteobacteria", "Verrucomicrobia"))
levels(dat.1$Phylum)
dat.1 <- droplevels(dat.1)
levels(dat.1$Phylum)

```

```{r phyla-level-plotting}
# Phyla plots with GAM smoother 
p.gam.phylum <- ggplot(dat.1, aes(x = treatment_days, y = Abundance, color = Phylum, group = Phylum)) +
  stat_smooth(method = "loess") +
  facet_grid(~treatment) +
  ylab("Relative Abundance") +
  geom_point(size = 1.25, alpha = 0.4)
p.gam.phylum

```

## Alpha diversity plotting

```{r alpha-diverstiy-GAM-plots}
alpha.div <- estimate_richness(ps2, measures = c("Observed", "Shannon"))
sd.1 <- as.data.frame(sample_data(ps2)) # Can not use original sd as samples have been removed
ps2.rich <- cbind(sd.1, alpha.div) # Bind alpha diversity columns to sample data

# Richness
p.rich <- ggplot(ps2.rich, aes(x = treatment_days, y = Observed, color = treatment, group = treatment)) +
  stat_smooth(method = "loess") +
  labs(y = "Richness", color = "Treatment") +
  geom_jitter(size = 2, alpha = 0.5, width = 0.2) +
  scale_color_manual(values = c("black", "chocolate", "green", "purple"))

# Shannon diversity
p.sd <- ggplot(ps2.rich, aes(x = treatment_days, y = Observed, color = treatment, group = treatment)) +
  stat_smooth(method = "loess") +
  labs(y = "Richness", color = "Treatment") +
  geom_jitter(size = 2, alpha = 0.5, width = 0.2) +
  scale_color_manual(values = c("black", "chocolate", "green", "purple"))

ggarrange(p.rich, p.sd, ncol = 2, labels = c("A)", "B)"))

```

## Ordination

```{r ordination}
#Ordination Analysis
ord.pcoa.uni <- ordinate(ps2, method = "PCoA", distance = "unifrac")
ord.pcoa.wuni <- ordinate(ps2, method = "PCoA", distance = "wunifrac")

```

## Beta diversity ordination plots ~ SurvivalStatus

```{r ordination-plots}
## Ordination plots all samples
# Unifrac
p.pcoa.uni <- plot_ordination(ps2, ord.pcoa.uni, color = "treatment", axes = c(1,2)) +
  geom_point(size = 2) +
  labs(title = "PCoA of UniFrac Distances", color = "Treatment") +
  facet_grid(~treatment_days) +
  stat_ellipse(type = "norm", geom = "polygon", alpha = 1/10, aes(fill = treatment))
p.pcoa.uni

# Weighted Unifrac
p.pcoa.wuni <- plot_ordination(ps2, ord.pcoa.wuni, color = "treatment", axes = c(1,2)) +
  geom_point(size = 2) +
  labs(title = "PCoA of wUniFrac Distances", color = "Treatment") +
  facet_grid(~treatment_days) +
  stat_ellipse(type = "norm", geom = "polygon", alpha = 1/10, aes(fill = treatment))
p.pcoa.wuni

ggarrange(p.pcoa.uni, p.pcoa.wuni, nrow = 2, labels = c("A)", "B)"))

```

## Group significance testing with ADONIS

```{r adonis-script}
# Set a random seed so that exact results can be reproduced
set.seed(10000)

# Function to run adonis test on a physeq object and a variable from metadata 
doadonis <- function(physeq, category) {
  bdist <- phyloseq::distance(physeq, "unifrac")
  col <- as(sample_data(physeq), "data.frame")[ ,category]
  
  # Adonis test
  adonis.bdist <- adonis(bdist ~ col)
  print("Adonis results:")
  print(adonis.bdist)
  
  # Homogeneity of dispersion test
  betatax = betadisper(bdist,col)
  p = permutest(betatax)
  print("Betadisper results:")
  print(p$tab)
}

doadonis(ps2, "treatment")

```

## **1) LRT: VEHICLE ~ METRO**

```{r differential-abundance-testing-vehicle-vs-metro-LRT}
# Subset infected Vehicle and Metro samples for the full treatment period
sample_data(ps2)$treatment
ps2.vehicle_metro <- ps2 %>%
  subset_samples(
    treatment != "Amp + Metro" &
    treatment != "Amp"
  )
sample_data(ps2.vehicle_metro)$treatment

# Test for taxa which at one or more time points after time 0 showed a treatment-specific effect
# Convert phyloseq object to DESeq2 table
ds.vehicle_metro.LRT <- phyloseq_to_deseq2(ps2.vehicle_metro, ~treatment + treatment_days + treatment:treatment_days)

# Run DESeq2
dds.vehicle_metro.LRT <- DESeq(ds.vehicle_metro.LRT, test="LRT", reduced = ~treatment + treatment_days)

# Tabulate results
res.dds.vehicle_metro.LRT <- results(dds.vehicle_metro.LRT)
res.dds.vehicle_metro.LRT$symbol <- mcols(dds.vehicle_metro.LRT)$symbol
summary(res.dds.vehicle_metro.LRT)
mcols(res.dds.vehicle_metro.LRT)
write.table(res.dds.vehicle_metro.LRT, file = "../results/deseq_vehicle_v_metro_LRT.txt", sep = "\t")

nrow(res.dds.vehicle_metro.LRT)
df.res <- as.data.frame(res.dds.vehicle_metro.LRT[ which(res.dds.vehicle_metro.LRT$padj < 0.05), ])
nrow(df.res)
df.res <- rownames_to_column(df.res, var = "ASV")
write.table(df.res, file = "../results/df_deseq2_results.txt", sep = "\t")

# Create appropriately formatted taxa table
# RDP
tax.table <- as.tibble(as.data.frame(tax_table(ps2)))
tax.table <- rownames_to_column(tax.table, var = "ASV")
df.rdp <- left_join(df.res, tax.table, by = "ASV")
colnames(df.rdp)

ggplot(df.rdp, aes(x = Phylum, y = log2FoldChange, color = Family)) +
  geom_jitter(size = 3, alpha = 0.7, width = 0.1) +
  geom_hline(yintercept = 0, lty = 2) +
  ylim(-35, 35)

```

```{r session-info}
# Display current R session information
sessionInfo()
```
